Based on your document from Citi and the "Trenda" EAP user guide, your vision of a **modular, granular forecasting shop** aligns perfectly with enterprise design. In a professional environment, separating each major component into a dedicated script allows for parallel processing, isolated debugging, and easy auditing.

Here is the breakdown of the scripts we will build, incorporating the **32-month training** and **4-month backtest** logic you requested.

---

### Phase 1: The Forecasting Engines

We will run each model type in its own separate script to keep the data granular. Each engine will fit its model on the first 32 months and predict the last 4 months of history to generate a "performance record."

* **`06_baseline_prophet.py`**
* **Function:** Trains on the first 32 months of each series and forecasts the remaining 4 months of history (backtest) plus 180 days into the future.
* **Data Structure:** It reads from `processed_data` and writes its specific results to a dedicated table (e.g., `prophet_results`).


* **`07_baseline_challenger.py`**
* **Function:** This is your "Challenger." We will use **XGBoost** (or another statistical model like ETS). It follows the exact same 32/4 split to ensure a fair comparison.
* **Data Structure:** Writes its granular predictions to `challenger_results`.



---

### Phase 2: The Tournament & Selection

Now that we have two sets of forecasts for the same 4-month historical period, we compare them to the actual "ground truth" utilization that already happened.

* **`08_model_competition.py` (The Tournament Judge)**
* **Function:** This script performs the "Backtest Audit." It compares the 4-month predictions from both models against the actual values in `processed_data`.
* **Metric:** It calculates the **MAPE** (Mean Absolute Percentage Error) for every server-resource pair.
* **Selection:** It flags the "Champion" model for each specific asset based on which one had the lower error in the recent past.



---

### Phase 3: Final Persistence & Reporting

With the winners crowned, we create the finalized "Golden Record" for enterprise planning.

* **`09_persist_final_forecast.py` (The Production Writer)**
* **Function:** It queries the competition results, fetches the winning model's 180-day future projection, and merges them into the final `forecasts` table.
* **Persistence:** This script ensures that when your business users look at a server, they see the most accurate prediction available.


* **`10_risk_reporting.py` (The Capacity Analyst)**
* **Function:** Scans the final forecasts for "High-Trust" breaches (upper bounds crossing 95% in the first 90 days).
* **Organizational View:** It maps these risks to your `business_hierarchy` (Departments and App Managers) to generate your exception lists.



---

### Phase 4: Visualization

* **`11_streamlit_dashboard.py` (The Executive View)**
* **Function:** A web-based portal to visualize the results.
* **Demo Content:**
* **Risk Heatmap:** Breakdown of breaches by department.
* **Tournament Summary:** Charts showing where Prophet won vs. where XGBoost won.
* **Deep Dive:** Interactive charts for individual "Risk" and "Normal" servers to build trust in the data.





### Next Step

Since you want to imitate a "real life shop," we should start by building **`06_baseline_prophet.py`** at full scale (2,000 servers).

**Would you like me to focus on the multiprocessing logic for this script first to ensure it can handle all 8,000 forecasts in a reasonable time?**

---

[A Complete Guide to Time Series Forecasting](https://www.youtube.com/watch?v=fxx_E0ojKrc)
This video provides a foundational overview of time series forecasting techniques in Python, which is essential for understanding the baseline and challenger models you are building for your capacity planning pipeline.