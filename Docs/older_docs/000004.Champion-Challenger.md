To build a professional "forecasting shop" that scales to your full inventory and includes a **Champion-Challenger** comparison, we will restructure the pipeline into the following six scripts. These scripts move the data from "Initial DNA" through "Tournament Selection" and finally to a "Dashboard."

### 1. `00_init_db.py`: The Infrastructure Architect

* **Main Function**: Purges old data and establishes the physical DuckDB database structure (`horizonscale_synth.db`).
* **Key Logic**: Seeds the enterprise inventory with 2,000 unique host UUIDs and assigns them a "Behavioral DNA" (Scenario and Variant) such as `STEADY_GROWTH` or `CAPACITY_BREACH`.
* **Standardization**: Forces all metadata to a consistent case (UPPER/lower) to ensure SQL joins work perfectly in downstream scripts.

### 2. `01_generate_master_parquet.py`: The Telemetry Engine

* **Main Function**: Translates the behavioral DNA into millions of rows of daily P95 utilization telemetry.
* **Key Logic**: Uses mathematical "Scenario Generators" (random walks, seasonality, and noise) to invent 3 years of history for CPU, Memory, Disk, and Network for every host.
* **Output**: Writes the "Data Lake" as a compressed Parquet file (`master_daily_2023_2025.parquet`).

### 3. `03_data_pipeline.py`: The Refinery

* **Main Function**: Acts as the ETL (Extract, Transform, Load) plant that prepares data for the models.
* **Key Logic**: Aggregates raw legacy sources, scrubs duplicates, ensures 100% timeline continuity, and joins telemetry with host metadata.
* **Output**: Populates the `processed_data` table, normalizing columns to `ds` (date) and `y` (utilization percentage).

### 4. `04_champion_challenger_forecast.py`: The Tournament (Production Scale)

* **Main Function**: This is the upgraded script that moves from 10 samples to the full 2,000-host inventory using multiple engines.
* **Key Logic**:
* **Backtesting**: Splits history into training data and a 30-day "test" window.
* **Tournament**: Runs **Prophet** (Champion) against **XGBoost** or **NeuralProphet** (Challenger).
* **Selection**: Calculates the **MAPE** (error) for both and crowns the winner for each specific server-resource pair.


* **Scaling**: Uses `multiprocessing` to run these 8,000 forecasts in parallel.

### 5. `05_risk_reporting.py`: The Analyst

* **Main Function**: Analyzes the stored forecasts to find business-critical capacity breaches.
* **Key Logic**: Filters the database for "High-Trust" risks (where the upper confidence interval `yhat_upper` exceeds 95% within the first 90 days).
* **Output**: Generates a CSV or SQL view summarizing which Departments (e.g., "Retail Banking") are at the highest risk of running out of capacity.

### 6. `06_streamlit_dashboard.py`: The Executive View

* **Main Function**: An interactive web interface to demo the forecasting shop's results.
* **Key Logic**: Connects to the DuckDB `forecasts` and `hosts` tables to provide:
* **Global Health**: A "Red/Yellow/Green" status for the whole fleet.
* **Drill-down**: Allows users to select any server and see the Prophet vs. Challenger comparison graph.
* **Model Audit**: Shows which model (Prophet or Challenger) is currently "winning" the most tournaments.



---

This modular approach is excellent for a "Production Forecasting Shop." Separating the engines into different scripts allows you to scale horizontally, debug model-specific issues without stopping the whole pipeline, and maintain a granular "Tournament Ledger" in your database.

Here is the refined blueprint for your scripts, organized to ensure data granularity and clear competition logic.

### 1. `06_baseline_prophet.py`: The Primary Engine

* **Main Function**: Processes the **full inventory** (2,000 servers Ã— 4 metrics) using Facebook Prophet.
* **Granular Output**: Saves predictions to a `prophet_forecasts` table.
* **Internal Backtest**: It must save a "Backtest" segment (predicting the last 30 days of known history) alongside the future 180-day projection to allow for error calculation.
* **Execution**: Uses `multiprocessing` to handle the heavy computational load.

### 2. `07_baseline_challenger.py`: The Secondary Engine (e.g., XGBoost)

* **Main Function**: Processes the same inventory using a different mathematical approach (like XGBoost or LightGBM).
* **Granular Output**: Saves results to a `challenger_forecasts` table.
* **Purpose**: To provide a second opinion on the data, specifically for servers where Prophet's linear/seasonal assumptions might fail (like "Burst" or "Black Swan" events).

### 3. `08_model_competition.py`: The Tournament Judge

* **Main Function**: Performs the **Champion-Challenger** comparison.
* **The Logic**:
1. Joins the `prophet_forecasts` and `challenger_forecasts` tables against the **actual** `processed_data` for that final 30-day "Backtest" window.
2. Calculates **MAPE** (Mean Absolute Percentage Error) for both models per host/resource.


* **Granular Output**: Writes to a `model_leaderboard` table, flagging which model is the "Champion" for every specific server.

### 4. `09_persist_final_forecast.py`: The Production Writer

* **Main Function**: Generates and persists the final "Golden Record" of the 6-month forecast.
* **The Logic**: It looks at the `model_leaderboard`, picks the winning model for each asset, and moves the corresponding 180-day projection into the final `forecasts` table.
* **High Trust**: This ensures your final production data is always based on the model that proved itself most accurate in the recent past.

### 5. `10_risk_reporting.py`: The Capacity Analyst

* **Main Function**: Scans the final `forecasts` table to identify business risks.
* **Key Filters**: Flags any resource where the high-trust upper bound () exceeds 95% within the first 90 days.
* **Organizational Mapping**: Joins results with the `business_hierarchy` table to attribute risks to specific Departments and App Codes.

### 6. `11_streamlit_dashboard.py`: The Executive Demo

* **Main Function**: A web-based UI for visualizing the "Forecasting Shop" results.
* **Interactive Features**:
* **Fleet Health**: High-level breach counts.
* **Model Audit**: A view showing where Prophet won vs. where the Challenger won.
* **Deep Dive**: Search for a specific Host UUID to see its interactive history and projection charts.



### Next Step: Scaling the Primary Engine

Since we are keeping it granular, we should start by upgrading your current logic into the **full-scale `06_baseline_prophet.py**`.

**Would you like me to provide the code for `06_baseline_prophet.py` that processes the full 2,000-server inventory in parallel and includes the backtesting window?**